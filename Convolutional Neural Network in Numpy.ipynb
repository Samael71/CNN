{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network in Numpy"
      ],
      "metadata": {
        "id": "uMgGthaJeTfv"
      },
      "id": "uMgGthaJeTfv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "t8e1pi1xebGr"
      },
      "id": "t8e1pi1xebGr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Libraries"
      ],
      "metadata": {
        "id": "ecma1Do5edky"
      },
      "id": "ecma1Do5edky"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9515a852",
      "metadata": {
        "id": "9515a852"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "596b7e8f",
      "metadata": {
        "id": "596b7e8f"
      },
      "source": [
        "### Data and preprocessing\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62813e3",
      "metadata": {
        "id": "e62813e3"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "#Importing the dataset\n",
        "dataset = pd.read_csv('train.csv')\n",
        "\n",
        "#Shuffling the dataset\n",
        "dataset = dataset.sample(frac=1, random_state=42)\n",
        "\n",
        "#Making sure the dataset is balanced\n",
        "train_data = pd.DataFrame()\n",
        "test_data = pd.DataFrame()\n",
        "for label in range (10):\n",
        "    train_label_data = dataset[dataset['label']==label].head(500)\n",
        "    test_label_data = dataset[dataset['label']==label].iloc[500:600,:]\n",
        "    train_data = pd.concat([train_data, train_label_data])\n",
        "    test_data = pd.concat([test_data, test_label_data])\n",
        "\n",
        "y_train_labels = train_data.iloc[:, 0].values\n",
        "y_test_labels = test_data.iloc[:, 0].values\n",
        "#One hot encoding the labels\n",
        "def one_hot_encoding(df, column_name):\n",
        "    one_hot_labels = pd.get_dummies(df[column_name])\n",
        "    dff = pd.concat([df,one_hot_labels], axis=1)\n",
        "    dff.drop(column_name, axis = 1, inplace = True) \n",
        "    return dff\n",
        "\n",
        "train_data = one_hot_encoding(train_data, 'label')\n",
        "test_data = one_hot_encoding(test_data, 'label')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2ff2641",
      "metadata": {
        "id": "c2ff2641"
      },
      "outputs": [],
      "source": [
        "train_data.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dd50fc9",
      "metadata": {
        "id": "3dd50fc9"
      },
      "outputs": [],
      "source": [
        "#From dataframes to numpy arrays\n",
        "X_train = train_data.iloc[:, 0:784].values\n",
        "X_test = test_data.iloc[:, 0:784].values\n",
        "y_train = train_data.iloc[:, 784:794].values\n",
        "y_test = test_data.iloc[:, 784:794].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59e1efce",
      "metadata": {
        "id": "59e1efce"
      },
      "outputs": [],
      "source": [
        "#Reshaping the data\n",
        "X_train_r = np.zeros((X_train.shape[0], 1, 28, 28), dtype=np.float64)\n",
        "X_test_r = np.zeros((X_test.shape[0], 1, 28, 28), dtype=np.float64)\n",
        "\n",
        "for im in range(X_train_r.shape[0]):\n",
        "    X = X_train[im,:]\n",
        "    X = np.ravel(X)\n",
        "    X = X.reshape((1, 28, 28))\n",
        "    X_train_r[im,0,:,:] = X\n",
        "X_train = X_train_r\n",
        "for im in range (X_test_r.shape[0]):\n",
        "    X = X_test[im,:]\n",
        "    X = np.ravel(X)\n",
        "    X = X.reshape((1, 28, 28))\n",
        "    X_test_r[im,0,:,:] = X\n",
        "X_test = X_test_r"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19f73de5",
      "metadata": {
        "id": "19f73de5"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the data"
      ],
      "metadata": {
        "id": "nATQJduHeofk"
      },
      "id": "nATQJduHeofk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb67fef6",
      "metadata": {
        "id": "cb67fef6"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "plt.imshow(X_train[800][0], interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdaf2499",
      "metadata": {
        "id": "cdaf2499"
      },
      "source": [
        "## Convolution and generating the Feature Map"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6ea5433",
      "metadata": {
        "id": "a6ea5433"
      },
      "source": [
        "### The filters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d1da84",
      "metadata": {
        "id": "01d1da84"
      },
      "outputs": [],
      "source": [
        "conv0 = np.random.randn(2,1,5,5)\n",
        "conv1 = conv0 * np.sqrt(1. / 5)\n",
        "conv1.shape[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff52c530",
      "metadata": {
        "id": "ff52c530"
      },
      "source": [
        "### Generating the feature map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca32891",
      "metadata": {
        "id": "8ca32891"
      },
      "outputs": [],
      "source": [
        "#The stride is 1\n",
        "def conv_layers(data, filters):\n",
        "    fearture_map_dim = len(data[0][0])-len(filters[0][0])+1\n",
        "    feature_map = np.zeros((data.shape[0], filters.shape[0], fearture_map_dim, fearture_map_dim))\n",
        "    for i in range (data.shape[0]):\n",
        "        image = data[i][0]\n",
        "        for j in range (filters.shape[0]):\n",
        "            filt = filters[j][0]\n",
        "            for d1 in range(fearture_map_dim):\n",
        "                s1 = d1\n",
        "                e1 = s1 + filters.shape[3]\n",
        "                for d2 in range(fearture_map_dim):\n",
        "                    s2 = d2\n",
        "                    e2 = s2 + filters.shape[3]\n",
        "                    im_section = image[s1:e1, s2:e2]\n",
        "                    feature_section = np.dot(im_section, filt)\n",
        "                    feature = np.sum(feature_section)\n",
        "                    feature_map[i][j][d1][d2] = feature\n",
        "    return feature_map"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the feature map"
      ],
      "metadata": {
        "id": "d1QVnaz9e3Wr"
      },
      "id": "d1QVnaz9e3Wr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fdd16b3",
      "metadata": {
        "id": "8fdd16b3"
      },
      "outputs": [],
      "source": [
        "feature_map = conv_layers(X_train, conv1)\n",
        "plt.imshow(feature_map[0][0], interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c24fbd9f",
      "metadata": {
        "id": "c24fbd9f"
      },
      "outputs": [],
      "source": [
        "feature_map.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76eb0bc6",
      "metadata": {
        "id": "76eb0bc6"
      },
      "source": [
        "## MaxPool layer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MaxPool operation"
      ],
      "metadata": {
        "id": "VI0p07_ZfZ_W"
      },
      "id": "VI0p07_ZfZ_W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f93b86b6",
      "metadata": {
        "id": "f93b86b6"
      },
      "outputs": [],
      "source": [
        "def MaxPool(feature_map, stride=2):\n",
        "    filter_dim = 2\n",
        "    output_dim = int((feature_map.shape[2]-filter_dim)/stride)+1\n",
        "    pooled_list = []\n",
        "    pooled_rows = np.empty(shape=[filter_dim, output_dim])\n",
        "    pooled_features = np.zeros((feature_map.shape[0], feature_map.shape[1], output_dim, output_dim))\n",
        "    \n",
        "    for i in range(feature_map.shape[0]):\n",
        "        for j in range(feature_map.shape[1]):\n",
        "            image = feature_map[i][j]\n",
        "            for d1 in range (0, feature_map.shape[2], stride):\n",
        "                start1 = d1 #if d1 ==0 else d1*2)\n",
        "                end1 = d1 + filter_dim\n",
        "                if end1<=feature_map.shape[2]:\n",
        "                    image_rectangle =image[start1:end1, :]\n",
        "                    for d2 in range (0, feature_map.shape[2], stride):\n",
        "                        start2 = d2 #if d2 ==0 else d1*2)\n",
        "                        end2 = d2 + filter_dim\n",
        "                        if end2<=feature_map.shape[2]:\n",
        "                            image_section = image_rectangle[:,start2:end2]\n",
        "                            pooled_section = np.amax(image_section)\n",
        "                            pooled_list.append(pooled_section)      \n",
        "                    if (pooled_list != []):\n",
        "                        pooled_row = np.array(pooled_list)\n",
        "                        pooled_row = np.reshape(pooled_list, (1, output_dim))\n",
        "                        pooled_rows = np.concatenate((pooled_rows, pooled_row), axis = 0)\n",
        "                        pooled_list.clear()\n",
        "            pooled_features[i][j] = pooled_rows[1:13,:]\n",
        "    return pooled_features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keeping the pooled pixels' indices in a vector"
      ],
      "metadata": {
        "id": "rLWisgDrfgVy"
      },
      "id": "rLWisgDrfgVy"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "501e9c82",
      "metadata": {
        "id": "501e9c82"
      },
      "outputs": [],
      "source": [
        "def maxpool_indices(input_image,stride=2,filter_height=2, filter_width=2):\n",
        "    positional_vector = []\n",
        "\n",
        "    for channel in range(input_image.shape[1]):\n",
        "        x = -1\n",
        "\n",
        "        chosen_image_channel = input_image[:,channel,:,:]\n",
        "        for height in range(0,chosen_image_channel.shape[1],stride):\n",
        "            if height+stride<=chosen_image_channel.shape[1]:\n",
        "                image_rectangle = chosen_image_channel[:,height:height+filter_height,:]\n",
        "                x = x+1\n",
        "                y = -1\n",
        "                \n",
        "                for width in range(0,image_rectangle.shape[2],stride):\n",
        "                    if width+stride<= image_rectangle.shape[2]:\n",
        "                        y = y+1\n",
        "                        \n",
        "                        image_square = image_rectangle[:,:,width:width+filter_width]\n",
        "                        \n",
        "                        a,b,c = np.unravel_index(image_square.argmax(),image_square.shape)\n",
        "\n",
        "                        \n",
        "                        positional_vector.append([0,channel,int(b)+height,int(c)+width,0,channel,x,y])\n",
        "    return positional_vector\n",
        "\n",
        "def maxpool_indices_multiple(input_image,stride=2,filter_height=2, filter_width=2):\n",
        "    positional_vector =[]\n",
        "    for i in range(input_image.shape[0]):\n",
        "        positional_vector.append(maxpool_indices(input_image[i:i+1,:,:,:],stride=2,filter_height=2,filter_width=2))\n",
        "    return positional_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspecting the result of the pooled features"
      ],
      "metadata": {
        "id": "2Aq0ji6MfoVJ"
      },
      "id": "2Aq0ji6MfoVJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb77b02",
      "metadata": {
        "id": "cdb77b02"
      },
      "outputs": [],
      "source": [
        "pooled_features1 = MaxPool(feature_map)\n",
        "pooled_features1.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c8e815",
      "metadata": {
        "id": "32c8e815"
      },
      "outputs": [],
      "source": [
        "plt.imshow(pooled_features1[0][0], interpolation='nearest')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37a7ed82",
      "metadata": {
        "id": "37a7ed82"
      },
      "source": [
        "## Useful functions:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc536f0",
      "metadata": {
        "id": "5cc536f0"
      },
      "source": [
        "### ReLu and ReLu Derivative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b8227bf",
      "metadata": {
        "id": "1b8227bf"
      },
      "outputs": [],
      "source": [
        "def ReLu(x):\n",
        "    return (x>0)*x\n",
        "\n",
        "def reluDerivative(x):\n",
        "    x1 = np.copy(x)\n",
        "    x1[x1<=0] = 0\n",
        "    x1[x1>0] = 1\n",
        "    return x1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee235810",
      "metadata": {
        "id": "ee235810"
      },
      "outputs": [],
      "source": [
        "feature_map = ReLu(feature_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a53e1396",
      "metadata": {
        "id": "a53e1396"
      },
      "source": [
        "### Im2col function: Transforming the image to col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9c0beb",
      "metadata": {
        "id": "5b9c0beb"
      },
      "outputs": [],
      "source": [
        "def im2col(X,conv1, stride, pad):\n",
        "    # Padding\n",
        "    X_padded = np.pad(X, ((0,0), (0,0), (pad, pad), (pad, pad)), mode='constant')\n",
        "    X = X_padded\n",
        "    new_height = int((X.shape[2]+(2*pad)-(conv1.shape[2]))/stride)+1\n",
        "    new_width =  int((X.shape[3]+(2*pad)-(conv1.shape[3]))/stride)+1\n",
        "    im2col_vector = np.zeros((X.shape[1]*conv1.shape[2]*conv1.shape[3],new_width*new_height*X.shape[0]))\n",
        "    c = 0\n",
        "    for position in range(X.shape[0]):\n",
        "\n",
        "        image_position = X[position,:,:,:]\n",
        "        for height in range(0,image_position.shape[1],stride):\n",
        "            image_rectangle = image_position[:,height:height+conv1.shape[2],:]\n",
        "            if image_rectangle.shape[1]<conv1.shape[2]:\n",
        "                continue\n",
        "            else:\n",
        "                for width in range(0,image_rectangle.shape[2],stride):\n",
        "                    image_square = image_rectangle[:,:,width:width+conv1.shape[3]]\n",
        "                    if image_square.shape[2]<conv1.shape[3]:\n",
        "                        continue\n",
        "                    else:\n",
        "                        im2col_vector[:,c:c+1]=image_square.reshape(-1,1)\n",
        "                        c = c+1         \n",
        "            \n",
        "    return(im2col_vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18472927",
      "metadata": {
        "id": "18472927"
      },
      "source": [
        "### Softmax:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f04ad93",
      "metadata": {
        "id": "6f04ad93"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=1, keepdims = True)\n",
        "def Softmax(x):\n",
        "    f = np.exp(x - np.max(x))  # shift values\n",
        "    return f / f.sum(axis=1, keepdims = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c7978f1",
      "metadata": {
        "id": "3c7978f1"
      },
      "source": [
        "### Reshaping the error layer for the conv:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b33014",
      "metadata": {
        "id": "d4b33014"
      },
      "outputs": [],
      "source": [
        "def error_layer_reshape(error_layer):\n",
        "    test_array = error_layer\n",
        "    test_array_new = np.zeros((test_array.shape[1],test_array.shape[0]*test_array.shape[2]*test_array.shape[3]))\n",
        "    for i in range(test_array_new.shape[0]):\n",
        "        test_array_new[i:i+1,:] = test_array[:,i:i+1,:,:].ravel()\n",
        "    return test_array_new"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f35302",
      "metadata": {
        "id": "e3f35302"
      },
      "source": [
        "## Initialising the network:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network Parameters"
      ],
      "metadata": {
        "id": "SXaVeuCCgga7"
      },
      "id": "SXaVeuCCgga7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a11ff10f",
      "metadata": {
        "id": "a11ff10f"
      },
      "outputs": [],
      "source": [
        "input_dim = pooled_features1.shape[1]*pooled_features1.shape[2]*pooled_features1.shape[3]\n",
        "hidden_dim1 = 128\n",
        "hidden_dim2 = 128\n",
        "output_dim = 10\n",
        "lambda_reg = 0.01\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Innit function"
      ],
      "metadata": {
        "id": "xhTRwMCHgmjn"
      },
      "id": "xhTRwMCHgmjn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc970b51",
      "metadata": {
        "id": "bc970b51"
      },
      "outputs": [],
      "source": [
        "def network_innit(input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "    model_dict = {}\n",
        "    W1 = np.random.randn(input_dim , hidden_dim1) * np.sqrt(1. / 5)\n",
        "    W2 = np.random.randn(hidden_dim1, hidden_dim2) * np.sqrt(1. / 5)\n",
        "    W3 = np.random.randn(hidden_dim2, output_dim) * np.sqrt(1. / 5)\n",
        "    b1 = np.zeros((1, hidden_dim1))\n",
        "    b2 = np.zeros((1, hidden_dim2))\n",
        "    b3 = np.zeros((1, output_dim))\n",
        "    model_dict['W1'] = W1\n",
        "    model_dict['W2'] = W2\n",
        "    model_dict['W3'] = W3\n",
        "    model_dict['b1'] = b1\n",
        "    model_dict['b2'] = b2\n",
        "    model_dict['b3'] = b3\n",
        "    return model_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9820ec9",
      "metadata": {
        "id": "c9820ec9"
      },
      "source": [
        "## Forward Propagation:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Dict"
      ],
      "metadata": {
        "id": "SZyRP2fLguGI"
      },
      "id": "SZyRP2fLguGI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02ba3f57",
      "metadata": {
        "id": "02ba3f57"
      },
      "outputs": [],
      "source": [
        "model_dict = network_innit(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "def get_param (model_dict):\n",
        "    W1 = model_dict['W1']\n",
        "    W2 = model_dict['W2']\n",
        "    W3 = model_dict['W3']\n",
        "    b1 = model_dict['b1']\n",
        "    b2 = model_dict['b2']\n",
        "    b3 = model_dict['b3']\n",
        "    return W1, W2, W3, b1, b2, b3"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Forward Function"
      ],
      "metadata": {
        "id": "yIIX6aQtgyuq"
      },
      "id": "yIIX6aQtgyuq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcbdcf6e",
      "metadata": {
        "id": "bcbdcf6e"
      },
      "outputs": [],
      "source": [
        "def forward(input_data, model_dict, filters):\n",
        "    W1, W2, W3, b1, b2, b3 = get_param(model_dict)\n",
        "    feature_map = conv_layers(input_data, filters)\n",
        "    activated_fm = ReLu(feature_map)\n",
        "    pooled_features = MaxPool(activated_fm)\n",
        "    max_indices = maxpool_indices_multiple(activated_fm,stride=2,filter_height=2, filter_width=2)\n",
        "    X = pooled_features.reshape(pooled_features.shape[0], -1)\n",
        "    z1 = X.dot(W1) + b1\n",
        "    a1 = ReLu(z1)\n",
        "    z2 = a1.dot(W2) + b2\n",
        "    a2 = ReLu(z2)\n",
        "    z3 = a2.dot(W3) + b3\n",
        "    predict = Softmax(z3)\n",
        "    return predict, z1, a1, z2, a2, z3, X, pooled_features, feature_map, max_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the final predictions"
      ],
      "metadata": {
        "id": "3VoF0w3Fg3QQ"
      },
      "id": "3VoF0w3Fg3QQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd2bac02",
      "metadata": {
        "id": "fd2bac02"
      },
      "outputs": [],
      "source": [
        "def get_predictions(predict):\n",
        "    return np.argmax(predict, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbd9474e",
      "metadata": {
        "id": "fbd9474e"
      },
      "source": [
        "## Loss: Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9efa5317",
      "metadata": {
        "id": "9efa5317"
      },
      "outputs": [],
      "source": [
        "def cross_entropy (predictions, labels, model_dict):\n",
        "    W1, W2, W3, b1, b2, b3 = get_param(model_dict)\n",
        "    m = np.zeros(predictions.shape[0])\n",
        "    for i, correct_predict in enumerate(labels):\n",
        "        m[i] = predictions[i][correct_predict]\n",
        "    log_prob = - np.log(m+0.0000000001)\n",
        "    loss = np.sum(log_prob)\n",
        "    reg_loss = (lambda_reg / 2)*(np.sum(np.sum(W1)+np.sum(W2)+np.sum(W3)))\n",
        "    loss += reg_loss\n",
        "    return float(loss/labels.shape[0]), log_prob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74a5dead",
      "metadata": {
        "id": "74a5dead"
      },
      "source": [
        "## Back Propagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0cf421f",
      "metadata": {
        "id": "e0cf421f"
      },
      "outputs": [],
      "source": [
        "def backpropagation(input_data, model_dict, filters, labels, labels_encoded, epochs):\n",
        "    for epoch in range (epochs):\n",
        "        W1, W2, W3, b1, b2, b3 = get_param(model_dict)\n",
        "        predictions, z1, a1, z2, a2, z3, X_f, X_maxpool, feature_map, max_indices  = forward(input_data, model_dict, filters)\n",
        "        predictions_f = get_predictions(predictions)\n",
        "        loss = cross_entropy (predictions, labels , model_dict)\n",
        "        \n",
        "        delta3 = predictions - labels_encoded\n",
        "        dW3 = np.dot(a2.T, delta3)\n",
        "        db3 = np.sum(delta3, axis=1, keepdims=True)\n",
        "        \n",
        "        delta2 = np.multiply(delta3.dot(W3.T), reluDerivative(a1@W2+b2))\n",
        "        dW2 = a1.T@delta2\n",
        "        db2 = np.sum(delta2, axis=1, keepdims=True)\n",
        "        \n",
        "        delta1 = np.multiply(delta2@W2.T, reluDerivative(X_f@W1+b1)) #(5000, 128)\n",
        "        dW1 = X_f.T@delta1\n",
        "        db1 = np.sum(delta1, axis=1, keepdims=True)\n",
        "        \n",
        "        delta0 = np.multiply(delta1@W1.T, 1.0)\n",
        "        \n",
        "        delta_maxpool = delta0.reshape(X_maxpool.shape)\n",
        "        \n",
        "        delta_conv = np.zeros(feature_map.shape)\n",
        "        for image in range(len(max_indices)):\n",
        "            indices = max_indices[image]\n",
        "            for p in indices:\n",
        "                delta_conv[image:image+1,p[1],p[2],p[3]] = delta_maxpool[image:image+1,p[5],p[6],p[7]]\n",
        "        \n",
        "        delta_activation = np.multiply(delta_conv, reluDerivative(feature_map))\n",
        "        #print(f'delta conv: {delta_conv.shape}')\n",
        "        #print(f'delta activation: {delta_activation.shape}')\n",
        "        #dconv1 = conv_layers(input_data, filters)\n",
        "        Im2Col = im2col(input_data, filters, stride=1, pad =0)\n",
        "        #print(f'Im2Col: {Im2Col.shape}')\n",
        "        delta_activation_reshape = error_layer_reshape(delta_activation)\n",
        "        #print(f'delta activation reshape: {delta_activation_reshape.shape}')\n",
        "        \n",
        "        conv1_delta = (delta_activation_reshape@Im2Col.T).reshape(2,1,5,5)\n",
        "        \n",
        "        ## Update Weights\n",
        "        filters = filters - learning_rate * conv1_delta\n",
        "        W1 = W1 - learning_rate * dW1\n",
        "        W2 = W2 - learning_rate * dW2\n",
        "        W3 = W3 - learning_rate * dW3\n",
        "        b1 = b1 - learning_rate * db1\n",
        "        b2 = b2 - learning_rate * db2\n",
        "        b3 = b3 - learning_rate * db3\n",
        "        \n",
        "        \n",
        "        model_dict['W1'] = W1\n",
        "        model_dict['W2'] = W2\n",
        "        model_dict['W3'] = W3\n",
        "        model_dict['b1'] = b1\n",
        "        model_dict['b2'] = b2\n",
        "        model_dict['b3'] = b3\n",
        "\n",
        "\n",
        "        if epoch%2==0:\n",
        "            print (f'---------------- Epoch: {epoch} ----------------')\n",
        "            print (f'Train Loss: {loss}')\n",
        "            labels_predict = predictions_f.tolist()\n",
        "            labels_predict = [int(value) for value in labels_predict]\n",
        "            #labels_df.loc[:,'label_predict'] = labels_predict\n",
        "            labels_df = pd.DataFrame(labels, columns = ['labels'])\n",
        "            labels_df.insert(1,'label_predict', labels_predict)\n",
        "            accuracy = np.sum(labels_df['labels']==labels_df['label_predict'])/labels.shape[0]\n",
        "            print(f'Train Accuracy: {round(accuracy*100,2)}%')\n",
        "\n",
        "    return model_dict    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c45a5d5",
      "metadata": {
        "id": "4c45a5d5"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_labels"
      ],
      "metadata": {
        "id": "3aL10MJMu19s"
      },
      "id": "3aL10MJMu19s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "310cbebd",
      "metadata": {
        "id": "310cbebd"
      },
      "outputs": [],
      "source": [
        "model_dict = network_innit(input_dim, hidden_dim1, hidden_dim2, output_dim)\n",
        "model = backpropagation(X_train, model_dict, conv1, y_train_labels, y_train, 40)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}